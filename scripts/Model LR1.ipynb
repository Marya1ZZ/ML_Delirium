{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd05ce53-74c7-46d9-abfe-6d948028f7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned dataset loaded successfully.\n",
      "Dataset Shape: (546028, 40)\n",
      "\n",
      "üîç Data Preview:\n",
      "   subject_id   hadm_id  admission_type      admission_location  \\\n",
      "0    10000032  22595853          URGENT  TRANSFER FROM HOSPITAL   \n",
      "1    10000032  22841357        EW EMER.          EMERGENCY ROOM   \n",
      "2    10000032  25742920        EW EMER.          EMERGENCY ROOM   \n",
      "3    10000032  29079034        EW EMER.          EMERGENCY ROOM   \n",
      "4    10000068  25022803  EU OBSERVATION          EMERGENCY ROOM   \n",
      "\n",
      "  discharge_location insurance marital_status   race  ed_time_spent  los_hosp  \\\n",
      "0               HOME  Medicaid        WIDOWED  WHITE          253.0  0.786111   \n",
      "1               HOME  Medicaid        WIDOWED  WHITE          337.0  1.015278   \n",
      "2            HOSPICE  Medicaid        WIDOWED  WHITE          286.0  1.754167   \n",
      "3               HOME  Medicaid        WIDOWED  WHITE          486.0  2.222222   \n",
      "4            UNKNOWN   UNKNOWN         SINGLE  WHITE          511.0  0.298611   \n",
      "\n",
      "   ... icu_airway_flag  icu_vent_mode_flag  icu_base_excess_flag  \\\n",
      "0  ...               0                   0                     0   \n",
      "1  ...               0                   0                     0   \n",
      "2  ...               0                   0                     0   \n",
      "3  ...               0                   0                     0   \n",
      "4  ...               0                   0                     0   \n",
      "\n",
      "  icu_lactate_flag  LabH_CRP_flag  LabH_Hb_flag  LabH_WBC_flag  \\\n",
      "0                0              0             0              0   \n",
      "1                0              0             0              0   \n",
      "2                0              0             0              0   \n",
      "3                0              0             0              0   \n",
      "4                0              0             0              0   \n",
      "\n",
      "   ed_time_missing  ed_missing_flag  age_group  \n",
      "0                0                0      30-60  \n",
      "1                0                0      30-60  \n",
      "2                0                0      30-60  \n",
      "3                0                0      30-60  \n",
      "4                0                0        <30  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "üîç Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 546028 entries, 0 to 546027\n",
      "Data columns (total 40 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   subject_id                 546028 non-null  int64  \n",
      " 1   hadm_id                    546028 non-null  int64  \n",
      " 2   admission_type             546028 non-null  object \n",
      " 3   admission_location         546028 non-null  object \n",
      " 4   discharge_location         546028 non-null  object \n",
      " 5   insurance                  546028 non-null  object \n",
      " 6   marital_status             546028 non-null  object \n",
      " 7   race                       546028 non-null  object \n",
      " 8   ed_time_spent              546028 non-null  float64\n",
      " 9   los_hosp                   546028 non-null  float64\n",
      " 10  gender                     546028 non-null  object \n",
      " 11  anchor_age                 546028 non-null  int64  \n",
      " 12  anchor_year                546028 non-null  int64  \n",
      " 13  primary_diagnosis          546028 non-null  object \n",
      " 14  palliative_care_flag       546028 non-null  int64  \n",
      " 15  delirium                   546028 non-null  int64  \n",
      " 16  cognitive_impairment_flag  546028 non-null  int64  \n",
      " 17  num_comorbidities          546028 non-null  int64  \n",
      " 18  prior_icu_admissions       546028 non-null  int64  \n",
      " 19  high_risk_med_flag         546028 non-null  float64\n",
      " 20  unique_high_risk_med       546028 non-null  float64\n",
      " 21  high_risk_med_count        546028 non-null  float64\n",
      " 22  drug                       546028 non-null  object \n",
      " 23  icu_map_flag               546028 non-null  int64  \n",
      " 24  icu_iabp_flag              546028 non-null  int64  \n",
      " 25  icu_pao2_flag              546028 non-null  int64  \n",
      " 26  icu_paco2_flag             546028 non-null  int64  \n",
      " 27  icu_peep_flag              546028 non-null  int64  \n",
      " 28  icu_ph_flag                546028 non-null  int64  \n",
      " 29  icu_fio2_flag              546028 non-null  int64  \n",
      " 30  icu_airway_flag            546028 non-null  int64  \n",
      " 31  icu_vent_mode_flag         546028 non-null  int64  \n",
      " 32  icu_base_excess_flag       546028 non-null  int64  \n",
      " 33  icu_lactate_flag           546028 non-null  int64  \n",
      " 34  LabH_CRP_flag              546028 non-null  int64  \n",
      " 35  LabH_Hb_flag               546028 non-null  int64  \n",
      " 36  LabH_WBC_flag              546028 non-null  int64  \n",
      " 37  ed_time_missing            546028 non-null  int64  \n",
      " 38  ed_missing_flag            546028 non-null  int64  \n",
      " 39  age_group                  546028 non-null  object \n",
      "dtypes: float64(5), int64(25), object(10)\n",
      "memory usage: 166.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#  Define the file path\n",
    "cleaned_file = \"D:/MIMIC-IV-Data-Pipeline/processed_data/mimic_cleaned_v8.csv.gz\"\n",
    "\n",
    "#  Load the dataset\n",
    "df = pd.read_csv(cleaned_file, compression=\"gzip\")\n",
    "\n",
    "#  Confirm successful load\n",
    "print(\" Cleaned dataset loaded successfully.\")\n",
    "print(f\"Dataset Shape: {df.shape}\")  # Check rows & columns\n",
    "print(\"\\nüîç Data Preview:\")\n",
    "print(df.head())  # Display first 5 rows\n",
    "\n",
    "# ‚úÖ Check data types and missing values\n",
    "print(\"\\nüîç Data Info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e5cc8b-df92-4636-b3f2-782879a99774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Categorical Variables in Data: ['admission_type', 'admission_location', 'discharge_location', 'insurance', 'marital_status', 'race', 'gender', 'primary_diagnosis', 'drug', 'age_group']\n",
      "üîç Data Types of Features:\n",
      "subject_id                     int64\n",
      "hadm_id                        int64\n",
      "admission_type                object\n",
      "admission_location            object\n",
      "discharge_location            object\n",
      "insurance                     object\n",
      "marital_status                object\n",
      "race                          object\n",
      "ed_time_spent                float64\n",
      "los_hosp                     float64\n",
      "gender                        object\n",
      "anchor_age                     int64\n",
      "anchor_year                    int64\n",
      "primary_diagnosis             object\n",
      "palliative_care_flag           int64\n",
      "cognitive_impairment_flag      int64\n",
      "num_comorbidities              int64\n",
      "prior_icu_admissions           int64\n",
      "high_risk_med_flag           float64\n",
      "unique_high_risk_med         float64\n",
      "high_risk_med_count          float64\n",
      "drug                          object\n",
      "icu_map_flag                   int64\n",
      "icu_iabp_flag                  int64\n",
      "icu_pao2_flag                  int64\n",
      "icu_paco2_flag                 int64\n",
      "icu_peep_flag                  int64\n",
      "icu_ph_flag                    int64\n",
      "icu_fio2_flag                  int64\n",
      "icu_airway_flag                int64\n",
      "icu_vent_mode_flag             int64\n",
      "icu_base_excess_flag           int64\n",
      "icu_lactate_flag               int64\n",
      "LabH_CRP_flag                  int64\n",
      "LabH_Hb_flag                   int64\n",
      "LabH_WBC_flag                  int64\n",
      "ed_time_missing                int64\n",
      "ed_missing_flag                int64\n",
      "age_group                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#  Define Target Variable\n",
    "target = \"delirium\"\n",
    "\n",
    "#  Separate Features (X) and Target (y)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "#  Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"üîç Categorical Variables in Data:\", categorical_cols)\n",
    "\n",
    "#  Check if Logistic Regression can handle them directly\n",
    "print(\"üîç Data Types of Features:\")\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a92e25-6d63-4a4d-9fde-b924f8cb1d40",
   "metadata": {},
   "source": [
    "üîç Categorical Features That Need Encoding\n",
    "Your dataset has 10 categorical features that need conversion:\n",
    "\n",
    "Nominal (unordered categories) ‚Üí One-Hot Encoding (OHE)\n",
    "admission_type\n",
    "admission_location\n",
    "discharge_location\n",
    "insurance\n",
    "marital_status\n",
    "race\n",
    "gender\n",
    "primary_diagnosis\n",
    "drug\n",
    "age_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55381468-d75c-40ab-9ddb-148843389be0",
   "metadata": {},
   "source": [
    "#this didnt work as high cardinality features gave an error: \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [\n",
    "    \"admission_type\", \"admission_location\", \"discharge_location\",\n",
    "    \"insurance\", \"marital_status\", \"race\", \"gender\",\n",
    "    \"primary_diagnosis\", \"drug\", \"age_group\"\n",
    "]\n",
    "\n",
    "#  Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)  # Drop first category to avoid multicollinearity\n",
    "\n",
    "#  Confirm new dataset shape\n",
    "print(\" One-Hot Encoding applied. New dataset shape:\", df_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c628e-e42d-43ac-af3e-9ebaf6ac78e5",
   "metadata": {},
   "source": [
    "high cardinality üìä   Use Target or Frequency Encoding for High-Cardinality Features\n",
    "Instead of One-Hot Encoding, we will: ‚úÖ Use Frequency Encoding for primary_diagnosis & drug (replace each category with its occurrence rate).\n",
    "‚úÖ Use One-Hot Encoding (OHE) for other categorical variables (small category sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d367fe8-2309-4eb7-b3b3-e25e1f143b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoding applied. New dataset shape: (546028, 108)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ‚úÖ Step 1: Frequency Encoding for High-Cardinality Features\n",
    "for col in [\"primary_diagnosis\", \"drug\"]:\n",
    "    freq_map = df[col].value_counts(normalize=True)  # Compute frequency of each category\n",
    "    df.loc[:, col] = df[col].map(freq_map)  # Replace categories with their frequency\n",
    "\n",
    "# ‚úÖ Step 2: One-Hot Encoding for Remaining Categorical Variables\n",
    "low_cardinality_cols = [\n",
    "    \"admission_type\", \"admission_location\", \"discharge_location\",\n",
    "    \"insurance\", \"marital_status\", \"race\", \"gender\", \"age_group\"\n",
    "]\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=low_cardinality_cols, drop_first=True)  # One-Hot Encode low-cardinality features\n",
    "\n",
    "# ‚úÖ Confirm dataset shape\n",
    "print(\"‚úÖ Encoding applied. New dataset shape:\", df_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297258e-3dc4-4dd8-bcbb-4de731e40331",
   "metadata": {},
   "source": [
    "Train & Evaluate Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a5fe33-ddb6-4448-9ff9-494f81fb6a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression Model Performance:\n",
      "Accuracy: 0.9818965990879622\n",
      "\n",
      "üîç Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    107248\n",
      "           1       0.42      0.03      0.05      1958\n",
      "\n",
      "    accuracy                           0.98    109206\n",
      "   macro avg       0.70      0.51      0.52    109206\n",
      "weighted avg       0.97      0.98      0.97    109206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#  Step 1: Define Features (X) and Target (y)\n",
    "target = \"delirium\"\n",
    "X = df_encoded.drop(columns=[target])\n",
    "y = df_encoded[target]\n",
    "\n",
    "#  Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#  Step 3: Standardize Numeric Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#  Step 4: Train Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=500, solver=\"lbfgs\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#  Step 5: Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "#  Step 6: Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "#  Step7: Evaluate Model Performance\n",
    "print(\" Logistic Regression Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972252ca-d96d-4140-bc61-7850e98f384f",
   "metadata": {},
   "source": [
    "baseline Logistic Regression model shows:\n",
    "\n",
    "‚úÖ High overall accuracy (98.19%)\n",
    "‚ö† Severe class imbalance (Delirium cases are rare, making the model biased towards predicting \"No Delirium\").\n",
    "‚ö† Poor recall & F1-score for delirium = 1 (very low sensitivity for identifying delirium cases).  \n",
    "\n",
    "üîç Key Issues & Insights\n",
    "1Ô∏è‚É£ Class Imbalance Problem:\n",
    "\n",
    "Delirium cases (1) are much fewer than non-delirium (0).\n",
    "Model predicts \"No Delirium\" nearly all the time, leading to high accuracy but poor recall for delirium = 1.\n",
    "2Ô∏è‚É£ Low Recall for delirium = 1 (3%)\n",
    "\n",
    "This means most actual delirium cases are not being predicted correctly.\n",
    "3Ô∏è‚É£ Potential Overfitting on Majority Class (delirium = 0)\n",
    "\n",
    "Since 98% of cases are non-delirium, the model learns to default to \"No Delirium\" to maximize accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81caffcc-ff72-4fa7-ac40-0eb09b11b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Convert confusion matrix to a list format for JSON storage\n",
    "conf_matrix_list = conf_matrix.tolist()\n",
    "\n",
    "\n",
    "# Define performance metrics\n",
    "# Define performance metrics with confusion matrix\n",
    "performance_metrics = {\n",
    "    \"Model\": \"LogisticReg_Base\",\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision (Delirium = 1)\": report[\"1\"][\"precision\"],\n",
    "    \"Recall (Delirium = 1)\": report[\"1\"][\"recall\"],\n",
    "    \"F1-Score (Delirium = 1)\": report[\"1\"][\"f1-score\"],\n",
    "    \"Confusion Matrix\": conf_matrix_list  # Store confusion matrix\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Compute ROC AUC Score\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Get probability estimates\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Generate ROC Curve data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Save ROC curve plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - LogisticReg_Base\")\n",
    "plt.legend()\n",
    "\n",
    "roc_plot_path = \"D:/MIMIC-IV-Data-Pipeline/ROC_LogisticReg_Base.png\"\n",
    "plt.savefig(roc_plot_path)\n",
    "plt.close()\n",
    "\n",
    "# Add ROC AUC score and plot path to performance metrics\n",
    "performance_metrics[\"ROC AUC Score\"] = roc_auc\n",
    "performance_metrics[\"ROC Curve Path\"] = roc_plot_path\n",
    "\n",
    "\n",
    "# Define file path\n",
    "performance_file = \"D:/MIMIC-IV-Data-Pipeline/model_performance.json\"\n",
    "\n",
    "# Load existing data if available\n",
    "if os.path.exists(performance_file):\n",
    "    with open(performance_file, \"r\") as file:\n",
    "        model_performance = json.load(file)\n",
    "else:\n",
    "    model_performance = []\n",
    "\n",
    "# Append new results\n",
    "model_performance.append(performance_metrics)\n",
    "\n",
    "# Save to file\n",
    "with open(performance_file, \"w\") as file:\n",
    "    json.dump(model_performance, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd28f7-964e-4e69-b80e-4308bbe481af",
   "metadata": {},
   "source": [
    "Address Class Imbalance & Improve Model Performance\n",
    "To improve performance, we can:\n",
    "\n",
    "üöÄ Choose a Strategy to Handle Class Imbalance: 1Ô∏è‚É£ Use Class Weights (balanced mode in Logistic Regression).\n",
    "\n",
    "This makes the model give more importance to delirium cases.\n",
    "2Ô∏è‚É£ Apply Oversampling (SMOTE) or Undersampling.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) generates synthetic delirium cases to balance the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55018b2a-b625-4d41-8b15-b3f437456553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Balanced Logistic Regression model performance saved successfully.\n",
      "‚úÖ Balanced Logistic Regression Model Performance:\n",
      "Accuracy: 0.8157610387707636\n",
      "\n",
      "üîç Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90    107248\n",
      "           1       0.08      0.84      0.14      1958\n",
      "\n",
      "    accuracy                           0.82    109206\n",
      "   macro avg       0.54      0.83      0.52    109206\n",
      "weighted avg       0.98      0.82      0.88    109206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain:  Balanced Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#  Step 1: Train Logistic Regression with Class Weights\n",
    "model_balanced = LogisticRegression(max_iter=500, solver=\"lbfgs\", class_weight=\"balanced\")\n",
    "model_balanced.fit(X_train_scaled, y_train)\n",
    "\n",
    "#  Step 2: Make Predictions\n",
    "y_pred_balanced = model_balanced.predict(X_test_scaled)\n",
    "\n",
    "#  Step 3: Evaluate Model Performance\n",
    "# Evaluate Balanced Logistic Regression Model Performance\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "report_balanced = classification_report(y_test, y_pred_balanced, output_dict=True)\n",
    "\n",
    "conf_matrix_balanced = confusion_matrix(y_test, y_pred_balanced).tolist()\n",
    "\n",
    "# Compute ROC AUC Score\n",
    "y_pred_proba_balanced = model_balanced.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_auc_balanced = roc_auc_score(y_test, y_pred_proba_balanced)\n",
    "fpr_balanced, tpr_balanced, _ = roc_curve(y_test, y_pred_proba_balanced)\n",
    "\n",
    "# Save ROC curve plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_balanced, tpr_balanced, label=f\"ROC Curve (AUC = {roc_auc_balanced:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - LogisticReg_Balanced\")\n",
    "plt.legend()\n",
    "\n",
    "roc_plot_path_balanced = \"D:/MIMIC-IV-Data-Pipeline/ROC_LogisticReg_Balanced.png\"\n",
    "plt.savefig(roc_plot_path_balanced)\n",
    "plt.close()\n",
    "\n",
    "# Define Balanced Logistic Regression performance metrics\n",
    "performance_metrics_LR_balanced = {\n",
    "    \"Model\": \"LogisticReg_Balanced\",\n",
    "    \"Accuracy\": accuracy_balanced,\n",
    "    \"Precision (Delirium = 1)\": report_balanced[\"1\"][\"precision\"],\n",
    "    \"Recall (Delirium = 1)\": report_balanced[\"1\"][\"recall\"],\n",
    "    \"F1-Score (Delirium = 1)\": report_balanced[\"1\"][\"f1-score\"],\n",
    "    \"Confusion Matrix\": conf_matrix_balanced,\n",
    "    \"ROC AUC Score\": roc_auc_balanced,\n",
    "    \"ROC Curve Path\": roc_plot_path_balanced\n",
    "}\n",
    "\n",
    "# Define file path\n",
    "performance_file = \"D:/MIMIC-IV-Data-Pipeline/model_performance.json\"\n",
    "\n",
    "# Load existing data if available\n",
    "if os.path.exists(performance_file):\n",
    "    with open(performance_file, \"r\") as file:\n",
    "        model_performance = json.load(file)\n",
    "else:\n",
    "    model_performance = []\n",
    "\n",
    "# Append new results\n",
    "model_performance.append(performance_metrics_LR_balanced)\n",
    "\n",
    "# Save to file\n",
    "with open(performance_file, \"w\") as file:\n",
    "    json.dump(model_performance, file, indent=4)\n",
    "\n",
    "print(\"üìä Balanced Logistic Regression model performance saved successfully.\")\n",
    "\n",
    "print(\"‚úÖ Balanced Logistic Regression Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_balanced))\n",
    "print(\"\\nüîç Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67ae57-c5d0-44f9-bfa4-5ce2b87c8a33",
   "metadata": {},
   "source": [
    "Evaluating Balanced Logistic Regression Model\n",
    "Your balanced Logistic Regression model shows:\n",
    "\n",
    "‚úÖ Drastic improvement in recall for delirium = 1 (84%) ‚Üí Now the model correctly identifies most delirium cases.\n",
    "‚ö† Significant drop in accuracy (81.6%) ‚Üí Model is misclassifying more non-delirium cases (delirium = 0).\n",
    "‚ö† Low precision for delirium = 1 (8%) ‚Üí Many predicted delirium cases are false positives.\n",
    "\n",
    "üìä Observations\n",
    "The model now catches most delirium cases (high recall) but at the cost of lowering precision and overall accuracy.\n",
    "False positives have increased, meaning more non-delirium patients are incorrectly classified as delirium.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de643bce-ef85-4c7f-8be2-d17ed8623a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SMOTE-Enhanced Logistic Regression Performance:\n",
      "Accuracy: 0.8147629251140047\n",
      "\n",
      "üîç Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90    107248\n",
      "           1       0.08      0.84      0.14      1958\n",
      "\n",
      "    accuracy                           0.81    109206\n",
      "   macro avg       0.54      0.83      0.52    109206\n",
      "weighted avg       0.98      0.81      0.88    109206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SMOTE : Apply SMOTE Oversampling to Balance the Dataset \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#  Step 1: Apply SMOTE to Balance Dataset\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)  # Auto balances the dataset\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "#  Step 2: Train Logistic Regression on SMOTE Data\n",
    "model_smote = LogisticRegression(max_iter=500, solver=\"lbfgs\")\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#  Step 3: Make Predictions\n",
    "y_pred_smote = model_smote.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "#  Step 4: Evaluate SMOTE Model Performance\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "report_smote = classification_report(y_test, y_pred_smote, output_dict=True)\n",
    "conf_matrix_smote = confusion_matrix(y_test, y_pred_smote).tolist()\n",
    "\n",
    "# Compute ROC AUC Score\n",
    "y_pred_proba_smote = model_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_auc_smote = roc_auc_score(y_test, y_pred_proba_smote)\n",
    "fpr_smote, tpr_smote, _ = roc_curve(y_test, y_pred_proba_smote)\n",
    "\n",
    "# Save ROC curve plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_smote, tpr_smote, label=f\"ROC Curve (AUC = {roc_auc_smote:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - LogisticReg_SMOTE\")\n",
    "plt.legend()\n",
    "\n",
    "roc_plot_path_smote = \"D:/MIMIC-IV-Data-Pipeline/ROC_LogisticReg_SMOTE.png\"\n",
    "plt.savefig(roc_plot_path_smote)\n",
    "plt.close()\n",
    "\n",
    "# Define SMOTE performance metrics\n",
    "performance_metrics_smote = {\n",
    "    \"Model\": \"LogisticReg_SMOTE\",\n",
    "    \"Accuracy\": accuracy_smote,\n",
    "    \"Precision (Delirium = 1)\": report_smote[\"1\"][\"precision\"],\n",
    "    \"Recall (Delirium = 1)\": report_smote[\"1\"][\"recall\"],\n",
    "    \"F1-Score (Delirium = 1)\": report_smote[\"1\"][\"f1-score\"],\n",
    "    \"Confusion Matrix\": conf_matrix_smote,\n",
    "    \"ROC AUC Score\": roc_auc_smote,\n",
    "    \"ROC Curve Path\": roc_plot_path_smote\n",
    "}\n",
    "\n",
    "# Define file path\n",
    "performance_file = \"D:/MIMIC-IV-Data-Pipeline/model_performance.json\"\n",
    "\n",
    "# Load existing data if available\n",
    "if os.path.exists(performance_file):\n",
    "    with open(performance_file, \"r\") as file:\n",
    "        model_performance = json.load(file)\n",
    "else:\n",
    "    model_performance = []\n",
    "\n",
    "# Append new results\n",
    "model_performance.append(performance_metrics_smote)\n",
    "\n",
    "# Save to file\n",
    "with open(performance_file, \"w\") as file:\n",
    "    json.dump(model_performance, file, indent=4)\n",
    "\n",
    "\n",
    "# ‚úÖ Step 5: Evaluate Model Performance\n",
    "print(\"‚úÖ SMOTE-Enhanced Logistic Regression Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_smote))\n",
    "print(\"\\nüîç Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f274cd5-e0f9-42cc-9d06-6bae5c331e89",
   "metadata": {},
   "source": [
    "SMOTE-enhanced model shows similar behavior to the balanced class-weight model:\n",
    "\n",
    "‚úÖ High recall for delirium = 1 (84%) ‚Üí The model correctly identifies most delirium cases.\n",
    "‚ö† Very low precision for delirium = 1 (8%) ‚Üí A high number of false positives.\n",
    "‚ö† Overall accuracy dropped to 81.5% ‚Üí Non-delirium cases are misclassified more often.\n",
    "üìä Observations & Comparison\n",
    "Model\tAccuracy\tPrecision (Delirium = 1)\tRecall (Delirium = 1)\tF1-Score (Delirium = 1)\n",
    "Baseline Logistic Regression\t98.2%\t42%\t3%\t5%\n",
    "Class-Weighted Logistic Regression\t81.6%\t8%\t84%\t14%\n",
    "SMOTE Logistic Regression\t81.5%\t8%\t84%\t14%\n",
    "Key Findings:\n",
    "Both Class-Weighted and SMOTE models increased recall but drastically reduced precision.\n",
    "Too many false positives make these models unreliable for real-world use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f73a0-ad3c-4efe-878c-3706b97a1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Feature Selection with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "809c5143-7ccf-44ee-904f-01f9ffa01afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edd311cf-3b69-4354-8ef8-960129b3c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using RFE:\n",
      " ['ed_time_spent', 'los_hosp', 'anchor_age', 'cognitive_impairment_flag', 'num_comorbidities', 'prior_icu_admissions', 'high_risk_med_flag', 'unique_high_risk_med', 'high_risk_med_count', 'drug', 'icu_vent_mode_flag', 'icu_base_excess_flag', 'icu_lactate_flag', 'admission_type_DIRECT OBSERVATION', 'admission_type_EU OBSERVATION', 'admission_type_EW EMER.', 'admission_type_OBSERVATION ADMIT', 'admission_type_SURGICAL SAME DAY ADMISSION', 'admission_type_URGENT', 'admission_location_EMERGENCY ROOM', 'admission_location_PACU', 'admission_location_PROCEDURE SITE', 'discharge_location_CHRONIC/LONG TERM ACUTE CARE', 'discharge_location_HOME', 'discharge_location_PSYCH FACILITY', 'discharge_location_REHAB', 'discharge_location_SKILLED NURSING FACILITY', 'discharge_location_UNKNOWN', 'gender_M', 'age_group_<30']\n"
     ]
    }
   ],
   "source": [
    "# Example: Keep top 30 features\n",
    "n_features_to_keep = 30\n",
    "\n",
    "# Create a base logistic regression model \n",
    "# Note: you can also specify solver='liblinear' or 'saga' if you want to use L1 penalty\n",
    "base_estimator = LogisticRegression(max_iter=500, solver='lbfgs')\n",
    "\n",
    "# Wrap the logistic regression with RFE\n",
    "rfe_selector = RFE(estimator=base_estimator, \n",
    "                   n_features_to_select=n_features_to_keep, \n",
    "                   step=1)\n",
    "\n",
    "# Fit the RFE object on your scaled training data\n",
    "rfe_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify which features are kept (True) or dropped (False)\n",
    "feature_support = rfe_selector.support_\n",
    "selected_features = [col for col, keep in zip(X_train.columns, feature_support) if keep]\n",
    "\n",
    "print(\"Selected features using RFE:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e278f434-d380-470a-a3c9-5bd2c253654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe = rfe_selector.transform(X_train_scaled)\n",
    "X_test_rfe = rfe_selector.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb9bf2bf-ec13-4db9-ba05-db630dffa57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE-Enhanced Logistic Regression Performance:\n",
      "Accuracy: 0.9818965990879622\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    107248\n",
      "           1       0.42      0.02      0.05      1958\n",
      "\n",
      "    accuracy                           0.98    109206\n",
      "   macro avg       0.70      0.51      0.52    109206\n",
      "weighted avg       0.97      0.98      0.97    109206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a new logistic regression on the selected features\n",
    "model_rfe = LogisticRegression(max_iter=500, solver='lbfgs')\n",
    "model_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rfe = model_rfe.predict(X_test_rfe)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
    "report_rfe = classification_report(y_test, y_pred_rfe)\n",
    "print(\"RFE-Enhanced Logistic Regression Performance:\")\n",
    "print(\"Accuracy:\", accuracy_rfe)\n",
    "print(\"\\nClassification Report:\\n\", report_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "171f2843-95a7-4479-a59e-e46f9a26d80a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 4. Create dictionary for performance metrics\u001b[39;00m\n\u001b[0;32m     27\u001b[0m performance_metrics_rfe \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogisticReg_RFE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_rfe,\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision (Delirium = 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mreport_rfe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall (Delirium = 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m: report_rfe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-Score (Delirium = 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m: report_rfe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m: conf_matrix_rfe,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC AUC Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc_auc_rfe,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC Curve Path\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc_plot_path_rfe,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Optional: Store which features RFE selected\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected Features\u001b[39m\u001b[38;5;124m\"\u001b[39m: selected_features  \n\u001b[0;32m     38\u001b[0m }\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 5. Append and save to model_performance.json\u001b[39;00m\n\u001b[0;32m     41\u001b[0m performance_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/MIMIC-IV-Data-Pipeline/model_performance.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# 1. Calculate confusion matrix\n",
    "conf_matrix_rfe = confusion_matrix(y_test, y_pred_rfe).tolist()\n",
    "\n",
    "# 2. Compute ROC AUC Score (optional)\n",
    "y_pred_proba_rfe = model_rfe.predict_proba(X_test_rfe)[:, 1]\n",
    "roc_auc_rfe = roc_auc_score(y_test, y_pred_proba_rfe)\n",
    "\n",
    "# 3. Generate ROC Curve data and save plot (optional)\n",
    "fpr_rfe, tpr_rfe, _ = roc_curve(y_test, y_pred_proba_rfe)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rfe, tpr_rfe, label=f\"ROC Curve (AUC = {roc_auc_rfe:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - LogisticReg_RFE\")\n",
    "plt.legend()\n",
    "\n",
    "roc_plot_path_rfe = \"D:/MIMIC-IV-Data-Pipeline/ROC_LogisticReg_RFE.png\"\n",
    "plt.savefig(roc_plot_path_rfe)\n",
    "plt.close()\n",
    "\n",
    "# 4. Create dictionary for performance metrics\n",
    "performance_metrics_rfe = {\n",
    "    \"Model\": \"LogisticReg_RFE\",\n",
    "    \"Accuracy\": accuracy_rfe,\n",
    "    \"Precision (Delirium = 1)\": report_rfe[\"1\"][\"precision\"],\n",
    "    \"Recall (Delirium = 1)\": report_rfe[\"1\"][\"recall\"],\n",
    "    \"F1-Score (Delirium = 1)\": report_rfe[\"1\"][\"f1-score\"],\n",
    "    \"Confusion Matrix\": conf_matrix_rfe,\n",
    "    \"ROC AUC Score\": roc_auc_rfe,\n",
    "    \"ROC Curve Path\": roc_plot_path_rfe,\n",
    "    # Optional: Store which features RFE selected\n",
    "    \"Selected Features\": selected_features  \n",
    "}\n",
    "\n",
    "# 5. Append and save to model_performance.json\n",
    "performance_file = \"D:/MIMIC-IV-Data-Pipeline/model_performance.json\"\n",
    "\n",
    "if os.path.exists(performance_file):\n",
    "    with open(performance_file, \"r\") as file:\n",
    "        model_performance = json.load(file)\n",
    "else:\n",
    "    model_performance = []\n",
    "\n",
    "model_performance.append(performance_metrics_rfe)\n",
    "\n",
    "with open(performance_file, \"w\") as file:\n",
    "    json.dump(model_performance, file, indent=4)\n",
    "\n",
    "print(\"‚úÖ RFE-Enhanced Logistic Regression performance saved successfully.\")\n",
    "print(\"Selected Features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16257d98-928c-4a74-97e9-aab7c4002714",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_rfe = classification_report(y_test, y_pred_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6695afdc-a3c6-4424-94a3-6b77eab295a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_rfe = classification_report(y_test, y_pred_rfe, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03634da4-55ad-43a2-ac55-fbd81f639f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9824916812569323, 'recall': 0.999375279725496, 'f1-score': 0.9908615644890658, 'support': 107248.0}, '1': {'precision': 0.41739130434782606, 'recall': 0.024514811031664963, 'f1-score': 0.04630969609261939, 'support': 1958.0}, 'accuracy': 0.9818965990879622, 'macro avg': {'precision': 0.6999414928023792, 'recall': 0.5119450453785804, 'f1-score': 0.5185856302908426, 'support': 109206.0}, 'weighted avg': {'precision': 0.9723597605017721, 'recall': 0.9818965990879622, 'f1-score': 0.9739262994091229, 'support': 109206.0}}\n"
     ]
    }
   ],
   "source": [
    "print(report_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5efa68f1-55fd-499a-8de4-8af2d505c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Dictionary Keys: dict_keys(['0', '1', 'accuracy', 'macro avg', 'weighted avg'])\n",
      "Precision (Delirium=1): 0.41739130434782606\n",
      "Recall (Delirium=1): 0.024514811031664963\n",
      "F1-Score (Delirium=1): 0.04630969609261939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Generate a classification report as a dictionary\n",
    "report_rfe = classification_report(y_test, y_pred_rfe, output_dict=True)\n",
    "\n",
    "# 2. Check the keys\n",
    "print(\"Classification Report Dictionary Keys:\", report_rfe.keys())\n",
    "\n",
    "# 3. Access the relevant class label\n",
    "#    Assuming your positive class is labeled '1'\n",
    "precision_1 = report_rfe[\"1\"][\"precision\"]\n",
    "recall_1 = report_rfe[\"1\"][\"recall\"]\n",
    "f1_1 = report_rfe[\"1\"][\"f1-score\"]\n",
    "\n",
    "print(\"Precision (Delirium=1):\", precision_1)\n",
    "print(\"Recall (Delirium=1):\", recall_1)\n",
    "print(\"F1-Score (Delirium=1):\", f1_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ac0a8eb-6e9f-465f-a9f8-a6cf166b6c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Performance metrics appended to JSON successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Example dictionary holding performance metrics\n",
    "performance_metrics_rfe = {\n",
    "    \"Model\": \"LogisticReg_RFE\",\n",
    "    \"Accuracy\": 0.92,  # example\n",
    "    \"Precision (Delirium = 1)\": 0.78,\n",
    "    \"Recall (Delirium = 1)\": 0.65,\n",
    "    \"F1-Score (Delirium = 1)\": 0.71,\n",
    "    \"Confusion Matrix\": [[100, 5], [10, 20]],  # example\n",
    "    \"ROC AUC Score\": 0.85,\n",
    "    \"ROC Curve Path\": \"D:/MIMIC-IV-Data-Pipeline/ROC_LogisticReg_RFE.png\",\n",
    "    # etc...\n",
    "}\n",
    "\n",
    "# Path to your JSON file\n",
    "performance_file = \"D:/MIMIC-IV-Data-Pipeline/model_performance.json\"\n",
    "\n",
    "# 1) Check if file exists, load it if it does, otherwise create empty list\n",
    "if os.path.exists(performance_file):\n",
    "    with open(performance_file, \"r\") as file:\n",
    "        model_performance = json.load(file)\n",
    "else:\n",
    "    model_performance = []\n",
    "\n",
    "# 2) Append new performance metrics to the list\n",
    "model_performance.append(performance_metrics_rfe)\n",
    "\n",
    "# 3) Write updated list back to the JSON file\n",
    "with open(performance_file, \"w\") as file:\n",
    "    json.dump(model_performance, file, indent=4)\n",
    "\n",
    "print(\"‚úÖ Performance metrics appended to JSON successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef39aba-25b6-447e-9e23-c103ec6b42d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
